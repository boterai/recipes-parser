"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ –∏ —Å–±–æ—Ä–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
"""
import os
import sys
import time
import json
import re
import random
import socket
import threading
from pathlib import Path
from urllib.parse import urlparse, urljoin
from typing import Set, Dict, List, Optional
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup

if __name__ == "__main__":
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from config.config import config
from src.stages.extract.recipe_extractor import RecipeExtractor
from src.stages.analyse.analyse import RecipeAnalyzer
from src.repositories.site import SiteRepository
from src.repositories.page import PageRepository
from src.models.site import Site
from src.models.page import Page
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SiteExplorer:
    """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤"""
    
    def __init__(self, base_url: str, debug_mode: bool = True, recipe_pattern: str = None,
                 max_errors: int = 3, max_urls_per_pattern: int = None, debug_port: int = None,
                 driver: webdriver.Chrome = None, custom_logger: logging.Logger = None, 
                 max_no_recipe_pages: Optional[int] = None, proxy: str = None):
        """
        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
            debug_mode: –ï—Å–ª–∏ True, –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –æ—Ç–∫—Ä—ã—Ç–æ–º—É Chrome —Å –æ—Ç–ª–∞–¥–∫–æ–π
            recipe_pattern: Regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ URL —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_errors: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥ –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
            max_urls_per_pattern: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –Ω–∞ –æ–¥–∏–Ω –ø–∞—Ç—Ç–µ—Ä–Ω (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
            debug_port: –ü–æ—Ä—Ç –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Chrome (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            driver: –ü–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä webdriver.Chrome (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
            custom_logger: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ª–æ–≥–≥–µ—Ä (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
            max_no_recipe_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞ –ø–æ–¥—Ä—è–¥ (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π). –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç –∏—Å—Å–ª–µ–¥–≤–æ–∞–Ω–∏–µ —Å–∞–π—Ç–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
            proxy: –ü—Ä–æ–∫—Å–∏ —Å–µ—Ä–≤–µ—Ä (—Ñ–æ—Ä–º–∞—Ç: host:port –∏–ª–∏ http://host:port). –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ config.PROXY
        """
        self.debug_mode = debug_mode
        self.debug_port = debug_port if debug_port is not None else config.PARSER_DEFAULT_CHROME_PORT
        self.proxy = proxy or config.PARSER_PROXY  # –ë–µ—Ä–µ–º –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∏–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        self.driver = driver
        self.recipe_regex = None
        self.request_count = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
        self.max_errors = max_errors
        self.max_urls_per_pattern = max_urls_per_pattern  # –õ–∏–º–∏—Ç URL –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ —Ä–µ—Ü–µ–ø—Ç–æ–≤–≤, —á—Ç–æ–±—ã –Ω–µ —Å–æ–±–∏—Ä–∞—Ç—å –∫—É—á–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
        self.analyzer = None
        self.site_repository = SiteRepository()
        self.page_repository = PageRepository()
        self.site = Site(
            base_url=base_url,
            pattern=recipe_pattern,
            name="",
        )
        if custom_logger:
            self.logger = custom_logger
        else:
            self.logger = logger
        self.site.set_url(base_url)
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è regex –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if self.site.pattern:
            try:
                self.recipe_regex = re.compile(self.site.pattern)
                self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–µ—Ü–µ–ø—Ç–æ–≤: {self.site.pattern}")
            except re.error as e:
                self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω: {e}")
                self.recipe_regex = None
        
        parsed_url = urlparse(base_url)
        self.base_domain = parsed_url.netloc.replace('www.', '')
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        self.visited_urls: Set[str] = set()
        self.url_patterns: Dict[str, List[str]] = {}  # –ø–∞—Ç—Ç–µ—Ä–Ω -> —Å–ø–∏—Å–æ–∫ URL
        self.failed_urls: Set[str] = set()
        self.referrer_map: Dict[str, str] = {}  # URL -> referrer URL (–æ—Ç–∫—É–¥–∞ –ø—Ä–∏—à–ª–∏)
        self.successful_referrers: Set[str] = set()  # URLs —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Ä–µ—Ü–µ–ø—Ç–∞–º
        self.exploration_queue: List[tuple] = []  # –û—á–µ—Ä–µ–¥—å URL –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: [(url, depth), ...]
        
        # –§–∞–π–ª—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.save_dir = os.path.join(config.PARSER_DIR, self.site.name,"exploration")
        os.makedirs(self.save_dir, exist_ok=True)
        
        self.state_file = os.path.join(self.save_dir, "exploration_state.json")
        self.patterns_file = os.path.join(self.save_dir, "url_patterns.json")

        site_orm = self.site_repository.create_or_get(self.site) # –Ω–∞–¥–æ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ site –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤—Å–µ —É–±—Ä–∞—Ç—å —Ç–∏–ø –ø–æ–ª—è –∏–∑ —Å–∞–π—Ç–∞ –∫–æ—Ç–æ—Ä—ã–µ 
        self.site = site_orm.to_pydantic()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL –∏–∑ –ë–î
        self.load_visited_urls_from_db()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ—Ü–µ–ø—Ç–æ–≤
        self.recipe_extractor = RecipeExtractor()
        self.max_no_recipe_pages: Optional[int] = max_no_recipe_pages 
        self.no_recipe_page_count: int = 0  # –°—á–µ—Ç—á–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞ –ø–æ–¥—Ä—è–¥

    def set_pattern(self, pattern: str):
        self.site.pattern = pattern
        try:
            self.recipe_regex = re.compile(pattern)
            self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–µ—Ü–µ–ø—Ç–æ–≤: {pattern}")
        except re.error as e:
            self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω: {e}")
            self.recipe_regex = None
    
    
    def load_visited_urls_from_db(self):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –∏–∑ –ë–î
        """
        pages_orm = self.page_repository.get_by_site(site_id=self.site.id)
        
        loaded_count = 0
        for page in pages_orm:
            if page.url:
                self.visited_urls.add(page.url)
                loaded_count += 1
                
                if not page.pattern:
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if page.pattern not in self.url_patterns:
                    self.url_patterns[page.pattern] = []
                if page.url not in self.url_patterns[page.pattern]:
                    self.url_patterns[page.pattern].append(page.url)
        
        if loaded_count > 0:
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –∏–∑ –ë–î")
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(self.url_patterns)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            return
        
        self.logger.info("–í –ë–î –Ω–µ—Ç —Ä–∞–Ω–µ–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –¥–ª—è —ç—Ç–æ–≥–æ —Å–∞–π—Ç–∞")
    
    def connect_to_chrome(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ"""

        if self.driver is not None:
             # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            try:
                _ = self.driver.current_url
                self.logger.info("‚úì –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±—Ä–∞—É–∑–µ—Ä—É")
                self.logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä webdriver.Chrome")
                return
            except Exception as e:
                self.logger.warning(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–µ—Å—Å–∏–µ–π: {e}")
                self.driver.quit()

        chrome_options = Options()
        
        if self.debug_mode:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Chrome –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø–æ—Ä—Ç—É
            if not self._is_chrome_running(self.debug_port):
                error_msg = (
                    f"\n{'='*60}\n"
                    f"–û–®–ò–ë–ö–ê: Chrome –Ω–µ –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {self.debug_port}\n\n"
                    f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ Chrome –∫–æ–º–∞–Ω–¥–æ–π:\n"
                    f"  google-chrome --remote-debugging-port={self.debug_port} "
                    f"--user-data-dir=./chrome_debug_{self.debug_port}\n\n"
                    f"–ò–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –ø–æ—Ä—Ç –Ω–µ –∑–∞–Ω—è—Ç:\n"
                    f"  lsof -i :{self.debug_port}\n"
                    f"{'='*60}\n"
                )
                self.logger.error(error_msg)
                raise WebDriverException(
                    f"Chrome –Ω–µ –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {self.debug_port}"
                )
            
            chrome_options.add_experimental_option(
                "debuggerAddress", 
                f"localhost:{self.debug_port}"
            )
            self.logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome –Ω–∞ –ø–æ—Ä—Ç—É {self.debug_port}")
        else:
            chrome_options.add_argument("--start-maximized")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            
            # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ Google (—É–±–∏—Ä–∞–µ—Ç –æ—à–∏–±–∫–∏ GCM)
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--disable-background-networking")
            chrome_options.add_argument("--disable-default-apps")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--log-level=3")
            
            # –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –º–µ–Ω—å—à–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(config.PARSER_DEFAULT_IMPLICIT_WAIT)
            self.driver.set_page_load_timeout(config.PARSER_DEFAULT_PAGE_LOAD_TIMEOUT)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            try:
                _ = self.driver.current_url
                self.logger.info("‚úì –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±—Ä–∞—É–∑–µ—Ä—É")
            except Exception as e:
                self.logger.warning(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–µ—Å—Å–∏–µ–π: {e}")
                self.driver.quit()
                raise
                
        except WebDriverException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±—Ä–∞—É–∑–µ—Ä—É: {e}")
            if self.debug_mode:
                self.logger.error(
                    f"\n–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Chrome –∑–∞–ø—É—â–µ–Ω:\n"
                    f"  ps aux | grep chrome | grep {self.debug_port}\n"
                )
            raise
    
    def _is_chrome_running(self, port: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—É—â–µ–Ω –ª–∏ Chrome –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ø–æ—Ä—Ç—É
        
        Args:
            port: –ü–æ—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
        Returns:
            True –µ—Å–ª–∏ Chrome –¥–æ—Å—Ç—É–ø–µ–Ω
        """        
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            return result == 0
        except Exception:
            return False
    
    def get_url_pattern(self, url: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ URL –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Å—Å—ã–ª–æ–∫
        
        Args:
            url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ü–∞—Ç—Ç–µ—Ä–Ω URL (—á–∏—Å–ª–∞ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ #, id –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ {id})
        """
        parsed = urlparse(url)
        path = parsed.path
        
        # –ó–∞–º–µ–Ω–∞ —á–∏—Å–µ–ª –Ω–∞ #
        pattern = re.sub(r'\d+', '#', path)
        
        # –ó–∞–º–µ–Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ {id}
        pattern = re.sub(r'[a-f0-9]{8,}', '{id}', pattern, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ trailing slash –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
        pattern = pattern.rstrip('/')
        
        return pattern or '/'
    
    def is_same_domain(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ URL —Ç–æ–º—É –∂–µ –¥–æ–º–µ–Ω—É –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å—É"""
        try:
            domain = urlparse(url).netloc.replace('www.', '')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞
            if domain != self.base_domain:
                return False
            
            return True
        except Exception:
            return False
    
    def is_recipe_url(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ URL —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
        """
        if not self.recipe_regex:
            return False
        
        try:
            parsed = urlparse(url)
            path = parsed.path
            return len(re.findall(self.site.pattern, path)) > 0
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ URL {url}: {e}")
            return False
    
    def check_and_extract_recipe(self, url: str, pattern: str, page_index: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ä–µ—Ü–µ–ø—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å —Ä–µ—Ü–µ–ø—Ç)
        
        Args:
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            save_html: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ HTML —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
            
        Returns:
            (is_recipe, confidence_score, recipe_data)
            - is_recipe: True –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ä–µ—Ü–µ–ø—Ç
            - confidence_score: —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0-100)
            - recipe_data: –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–∞ –∏–ª–∏ None
        """
        language = self.driver.execute_script("return document.documentElement.lang") or 'unknown'
        page = Page(site_id=self.site.id, 
                    url=url, 
                    pattern=pattern, 
                    html_path=self.save_page_as_file(pattern, page_index),
                    title=self.driver.title,
                    language=language)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–∞
        recipe_data: Optional[Page] = self.recipe_extractor.extract_and_update_page(page)
        if not recipe_data:
            self.logger.info(f"  ‚úó –†–µ—Ü–µ–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ {url}")
            self.no_recipe_page_count += 1
            return False

        if (self.site.language is None or self.site.language != language) and language != 'unknown':
            self.site.language = language
            try:
                self.site_repository.update_language(site_id=self.site.id, language=language)
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —è–∑—ã–∫–∞ —Å–∞–π—Ç–∞ –≤ –ë–î: {e}")
        
        if recipe_data.is_recipe is False:
            self.logger.info(f"  ‚úó –†–µ—Ü–µ–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ {url}")
            self.no_recipe_page_count += 1
            return False

        try:
            image_urls = recipe_data.image_urls.split(",") if recipe_data.image_urls else []
            self.page_repository.create_or_update_with_images(recipe_data, image_urls=image_urls)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ë–î: {e}")
            self.no_recipe_page_count += 1
            return False
        
        dish_name = recipe_data.dish_name or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        self.logger.info(f"  ‚úì –†–µ—Ü–µ–ø—Ç '{dish_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î")
        self.no_recipe_page_count = 0 # —Å–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞
        return True
    
    def should_explore_url(self, url: str, ignore_visited: bool = False) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π URL
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏     
            ignore_visited: –ï—Å–ª–∏ True, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Å—ã–ª–æ–∫)       
        Returns:
            True –µ—Å–ª–∏ URL –Ω—É–∂–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç—å
        """
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –ø–æ—Å–µ—â–∞–ª–∏
        if url in self.visited_urls and ignore_visited is False:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ URL –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω
        if self.max_urls_per_pattern is not None and self.recipe_regex is None:
            pattern = self.get_url_pattern(url)
            current_count = len(self.url_patterns.get(pattern, []))
            if current_count >= self.max_urls_per_pattern:
                self.logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ URL: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {self.max_urls_per_pattern} –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞ {pattern}")
                return False
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã
        if re.search(r'\.(jpg|jpeg|png|gif|pdf|zip|mp4|avi|css|js)$', url, re.IGNORECASE):
            return False
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        skip_patterns = [
            r'/answers'
            r'/login',
            r'/register',
            r'/signup',
            r'/blog',
            r'/news',
            r'/forum',
            r'/admin',
            r'/dashboard',
            r'/logout',
            r'/user'
            r'/signin',
            r'/auth',
            r'/account',
            r'/profile',
            r'/settings',
            r'/about',
            r'/contact',
            r'/privacy',
            r'/terms',
            r'/cookie',
            r'/newsletter',
            r'/subscribe',
            r'/unsubscribe',
            r'/cart',
            r'/checkout',
            r'/order',
            r'/feedback',
            r'/help',
            r'/support',
            r'/faq',
            r'/advertise',
            r'/careers',
            r'/jobs',
            r'/sitemap',
            r'/404',
            r'/500'
            r'/products',

        ]
        
        parsed = urlparse(url)
        path_lower = parsed.path.lower()
        
        for skip_pattern in skip_patterns:
            if re.search(skip_pattern, path_lower):
                self.logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ —Å–ª—É–∂–µ–±–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
                return False
        return True
    
    def get_url_priority(self, url: str) -> int:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ URL –¥–ª—è –æ–±—Ö–æ–¥–∞

        Args:
            url: URL –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–º–µ–Ω—å—à–µ = –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0 (–Ω–∞–∏–≤—ã—Å—à–∏–π): URL —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º —Ä–µ—Ü–µ–ø—Ç–∞
        if self.is_recipe_url(url):
            return 0
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: URL —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Ä–µ—Ü–µ–ø—Ç–∞–º
        referrer = self.referrer_map.get(url)
        if referrer and referrer in self.successful_referrers:
            return 1
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –æ—Å—Ç–∞–ª—å–Ω—ã–µ URL
        return 2
    
    def slow_scroll_page(self, quick_mode: bool = False):
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            quick_mode: –ï—Å–ª–∏ True, –¥–µ–ª–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–∫—Ä—É—Ç–∫—É (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        """
        try:
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if quick_mode:
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞: 2-3 —à–∞–≥–∞ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø–∞—É–∑–∞–º–∏
                num_scrolls = random.randint(2, 3)
                scroll_step = total_height // num_scrolls
                
                current_position = 0
                for i in range(num_scrolls):
                    current_position += scroll_step
                    self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                    time.sleep(random.uniform(0.1, 0.5))
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –≤ –∫–æ–Ω–µ—Ü
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(0.1, 0.3))
            else:
                # –û–±—ã—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
                num_scrolls = random.randint(3, 5)
                scroll_step = total_height // num_scrolls
                
                current_position = 0
                for i in range(num_scrolls):
                    current_position += scroll_step
                    self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                    time.sleep(random.uniform(0.2, 0.4))
                
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(0.25, 0.5))
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–∫—Ä—É—Ç–∫–µ: {e}")


    def save_page_as_file(self, pattern: str, page_index: int) -> str:
        """        
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
        Args:
            pattern: –ü–∞—Ç—Ç–µ—Ä–Ω URL
            page_index: –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É HTML
        """

        html_content = self.driver.page_source
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        safe_pattern = pattern.replace('/', '_').replace('#', 'N').replace('{', '').replace('}', '').strip('_')
        if not safe_pattern:
            safe_pattern = 'index'
        
        filename = f"{safe_pattern}_{page_index}.html"
        filepath = os.path.join(self.save_dir,filename)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return filepath

    
    def save_page_html(self, url: str, pattern: str, page_index: int):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ë–î
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            pattern: –ü–∞—Ç—Ç–µ—Ä–Ω URL
            page_index: –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        """
            
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        title = self.driver.title
        language = self.driver.execute_script("return document.documentElement.lang") or 'unknown'
        filepath = self.save_page_as_file(pattern, page_index)
        filename = os.path.basename(filepath)
        page_orm = self.page_repository.create_or_update(
            Page(
                site_id=self.site.id,
                url=url,
                pattern=pattern,
                title=title,
                language=language,
                html_path=os.path.relpath(filepath)
            ))
    
        if page_orm.id:
            self.logger.info(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} (DB ID: {page_orm.id})")
        else:
            self.logger.info(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} (–ë–î: –æ—à–∏–±–∫–∞)")


    
    def extract_links(self) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏)
        –î–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ extract_links_with_priority()
        """
        try:
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(self.driver.current_url, href)
                
                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç —è–∫–æ—Ä–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                clean_url = absolute_url.split('#')[0].split('?')[0]
                
                if clean_url and self.is_same_domain(clean_url):
                    links.append(clean_url)
            
            return list(set(links))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")
            return []
    

    def extract_links_with_priority(self) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π —É—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
        –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è:
        1. –°—Å—ã–ª–∫–∏ –æ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        2. –°—Å—ã–ª–∫–∏ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º URL, –æ—Ç–ª–∏—á–Ω—ã–º –æ—Ç —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è)
        
        Returns:
            –°–ø–∏—Å–æ–∫ URL (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –Ω–∞—á–∞–ª–µ)
        """
        try:
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            priority_links = []  # –û—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            different_pattern_links = []  # –° –¥—Ä—É–≥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
            regular_links = []  # –û—Å—Ç–∞–ª—å–Ω—ã–µ
            seen_urls = set()
            
            current_url = self.driver.current_url
            is_successful_source = current_url in self.successful_referrers
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Ç–µ–∫—É—â–µ–≥–æ URL (—á–∏—Å–ª–∞ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ #)
            current_pattern = self.get_url_pattern(current_url)
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(current_url, href)
                
                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç —è–∫–æ—Ä–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                clean_url = absolute_url.split('#')[0].split('?')[0]
                
                if not (clean_url and self.is_same_domain(clean_url)):
                    continue
                
                if clean_url in seen_urls:
                    continue
                
                seen_urls.add(clean_url)
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Å—Å—ã–ª–∫–∏
                link_pattern = self.get_url_pattern(clean_url)
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –°—Å—ã–ª–∫–∏ –æ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                if is_successful_source:
                    priority_links.append(clean_url)
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –°—Å—ã–ª–∫–∏ —Å –¥—Ä—É–≥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º (–¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è)
                elif link_pattern != current_pattern:
                    different_pattern_links.append(clean_url)
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –û—Å—Ç–∞–ª—å–Ω—ã–µ (—Å —Ç–∞–∫–∏–º –∂–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º)
                else:
                    regular_links.append(clean_url)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            result = priority_links + different_pattern_links + regular_links
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if different_pattern_links:
                self.logger.debug(f"  –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(different_pattern_links)} —Å—Å—ã–ª–æ–∫ —Å –¥—Ä—É–≥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º (—Ç–µ–∫—É—â–∏–π: {current_pattern})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: {e}")
            return []
    
    
    def  export_state(self) -> dict:
        """–≠–∫—Å–ø–æ—Ä—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –¥—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º explorer
        """
        return {
            'base_url': self.site.base_url,
            'recipe_pattern': self.site.pattern,
            'visited_urls': list(self.visited_urls),
            'url_patterns': dict(self.url_patterns),
            'failed_urls': list(self.failed_urls),
            'referrer_map': dict(self.referrer_map),
            'successful_referrers': list(self.successful_referrers),
            'exploration_queue': list(self.exploration_queue),
            'request_count': self.request_count,
            'site_id': self.site.id,
            'site_name': self.site.name,
            'exported_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def add_helper_urls(self, urls: List[str], depth: int = 0):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ URL –≤ –æ—á–µ—Ä–µ–¥—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            depth: –ù–∞—á–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–ª—è —ç—Ç–∏—Ö URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0)
        """
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—á–µ—Ä–µ–¥—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        self.exploration_queue.sort(key=lambda x: self.get_url_priority(x[0]))

        added_count = 0
        for url in urls:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞
            if not self.is_same_domain(url):
                self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω URL –¥—Ä—É–≥–æ–≥–æ –¥–æ–º–µ–Ω–∞: {url}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –µ—â–µ –Ω–µ –ø–æ—Å–µ—â–µ–Ω –∏ –Ω–µ –≤ –æ—á–µ—Ä–µ–¥–∏
            self.exploration_queue.insert(0, (url, depth))
            added_count += 1
            self.logger.info(f"  + –î–æ–±–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–æ: {url}")
        
        
        self.logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö URL –≤ –æ—á–µ—Ä–µ–¥—å")
        self.logger.info(f"–í—Å–µ–≥–æ –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.exploration_queue)} URL")
    
    def import_state(self, state: dict):
        """–ò–º–ø–æ—Ä—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ –¥—Ä—É–≥–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        
        Args:
            state: –°–ª–æ–≤–∞—Ä—å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ export_state()
        """
        self.visited_urls = set(state.get('visited_urls', []))
        self.url_patterns = {k: v for k, v in state.get('url_patterns', {}).items()}
        self.failed_urls = set(state.get('failed_urls', []))
        self.referrer_map = dict(state.get('referrer_map', {}))
        self.successful_referrers = set(state.get('successful_referrers', []))
        self.exploration_queue = [tuple(item) for item in state.get('exploration_queue', [])]
        self.request_count = state.get('request_count', 0)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º regex –ø–∞—Ç—Ç–µ—Ä–Ω –µ—Å–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è
        new_pattern = state.get('recipe_pattern')
        if new_pattern and new_pattern != self.site.pattern:
            self.site.pattern = new_pattern
            try:
                self.recipe_regex = re.compile(new_pattern)
                self.logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω regex –ø–∞—Ç—Ç–µ—Ä–Ω: {new_pattern}")
            except re.error as e:
                self.logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ: {e}")
        
        self.logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL, "
                   f"{len(self.url_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, {len(self.exploration_queue)} URL –≤ –æ—á–µ—Ä–µ–¥–∏, "
                   f"{self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        state = self.export_state()
        
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        patterns_data = {
            'patterns': dict(self.url_patterns),
            'total_patterns': len(self.url_patterns),
            'total_unique_urls': sum(len(urls) for urls in self.url_patterns.values())
        }
        
        with open(self.patterns_file, 'w', encoding='utf-8') as f:
            json.dump(patterns_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–æ, {len(self.url_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    
    def load_state(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.state_file):
            self.logger.info("–§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
            return False
        
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)
            
            self.import_state(state)
            
            self.logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
            self.logger.info(f"  –ü–æ—Å–µ—â–µ–Ω–æ URL: {len(self.visited_urls)}")
            self.logger.info(f"  –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.url_patterns)}")
            self.logger.info(f"  URL –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.exploration_queue)}")
            self.logger.info(f"  –£—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.successful_referrers)}")
            self.logger.info(f"  –û—à–∏–±–æ–∫: {len(self.failed_urls)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            return False
        
    def should_extract_recipe(self, current_url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            current_url: URL —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            check_url: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç
        """
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –∑–∞–¥–∞–Ω - –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ–≥–¥–∞
        if self.recipe_regex is None:
            return True
        
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –∑–∞–¥–∞–Ω - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        return self.is_recipe_url(current_url)
        
    def mark_page_as_successful(self, current_url: str):
        """
        –û—Ç–º–µ—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ —É—Å–ø–µ—à–Ω—É—é (—Å —Ä–µ—Ü–µ–ø—Ç–æ–º) –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å, —Å—Ç–∞–≤—è URL –æ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –Ω–∞—á–∞–ª–æ
        """
        referrer = self.referrer_map.get(current_url)
        if referrer:
            self.successful_referrers.add(referrer)
            self.logger.info(f"  ‚úì –ò—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–º–µ—á–µ–Ω –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π: {referrer}")
            
            # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å: URL –æ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ - –≤ –Ω–∞—á–∞–ª–æ
            if self.exploration_queue:
                priority_urls = []
                other_urls = []
                
                for url_tuple in self.exploration_queue:
                    url, _ = url_tuple
                    if self.referrer_map.get(url) == referrer:
                        priority_urls.append(url_tuple)
                    else:
                        other_urls.append(url_tuple)
                
                if priority_urls:
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ URL –≤–ø–µ—Ä–µ–¥, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ
                    self.exploration_queue = priority_urls + other_urls
                    self.logger.info(f"  ‚Üë {len(priority_urls)} URL –æ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–µ—Ä–µ–¥–≤–∏–Ω—É—Ç—ã –≤ –Ω–∞—á–∞–ª–æ –æ—á–µ—Ä–µ–¥–∏")
    
    def _navigate_with_timeout(self, url: str, timeout: int = 90) -> bool:
        """
        –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º timeout —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫
        
        Args:
            url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –µ—Å–ª–∏ timeout
        """
        load_complete = threading.Event()
        navigation_error = [None]  # –î–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏–∑ –ø–æ—Ç–æ–∫–∞
        
        def navigate():
            try:
                self.driver.get(url)
                load_complete.set()
            except Exception as e:
                navigation_error[0] = e
                load_complete.set()
        
        # –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        nav_thread = threading.Thread(target=navigate, daemon=True)
        nav_thread.start()
        
        # –û–∂–∏–¥–∞–Ω–∏–µ —Å timeout
        if not load_complete.wait(timeout=timeout):
            # Timeout - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            self.logger.warning(f"‚è± Timeout {timeout}s –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º")
            try:
                self.driver.execute_script("window.stop();")
            except Exception:
                pass
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
        if navigation_error[0]:
            if isinstance(navigation_error[0], TimeoutException):
                self.logger.warning("‚è± Selenium TimeoutException –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ")
                try:
                    self.driver.execute_script("window.stop();")
                except Exception:
                    pass
                return False
            else:
                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º
                raise navigation_error[0]
        
        return True
    
    def explore(self, max_urls: int = 100, max_depth: int = 3, session_urls: bool = True, 
                check_pages_with_extractor:bool = False, check_url: bool = False) -> int:
        """
        –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
        
        Args:
            max_urls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
            session_urls: –ï—Å–ª–∏ True, —Ç–æ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL –ø—Ä–∏ –ø–æ–¥—Å—á—Ç–µ–µ max urls
            check_pages_with_extractor: –ï—Å–ª–∏ True, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º —Ä–µ—Ü–µ–ø—Ç–æ–≤
            check_url: –ï—Å–ª–∏ True, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—ã–π –Ω–∞ —Ä–µ–¥–∂–µ–∫—Å –ø–∞—Ç—Ç–µ—Ä–Ω –ø–µ—Ä–µ–¥ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π (–ø–∞—Ä–∞–º—Ç–µ—Ä –∫–∞—Å–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏)
        Returns:
            urls_explored: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        """
        self.logger.info(f"–ù–∞—á–∞–ª–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞: {self.site.base_url}")
        self.logger.info(f"–¶–µ–ª—å: –Ω–∞–π—Ç–∏ –¥–æ {max_urls} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ URL")
        
        # –û—á–µ—Ä–µ–¥—å URL –¥–ª—è –æ–±—Ö–æ–¥–∞: (url, depth)
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –æ—á–µ—Ä–µ–¥—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ –Ω–∞—á–∏–Ω–∞–µ–º —Å base_url
        if self.exploration_queue:
            queue = list(self.exploration_queue)
            self.logger.info(f"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –æ—á–µ—Ä–µ–¥—å—é: {len(queue)} URL")
        else:
            queue = [(self.site.base_url, 0)]
            self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ")
        
        urls_explored = len(self.visited_urls)

        if session_urls:
            urls_explored = 0  # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        initial_strategy = "–≥–ª—É–±–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω)" if self.recipe_regex is None else "—à–∏—Ä–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω)"
        self.logger.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ö–æ–¥–∞: {initial_strategy}")

        err_count = 0  # –°—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥
        last_strategy = self.recipe_regex is not None  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π

        while queue and urls_explored < max_urls:
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞ –ø–æ–¥—Ä—è–¥
            if self.max_no_recipe_pages and self.no_recipe_page_count >= self.max_no_recipe_pages:
                self.logger.info(f"üö´ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç {self.max_no_recipe_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞ –ø–æ–¥—Ä—è–¥, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
                break
            # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ—Ç - –∏–¥–µ–º –≤–≥–ª—É–±—å (LIFO), –∏–Ω–∞—á–µ –≤—à–∏—Ä—å (FIFO)
            has_recipe_pattern = self.recipe_regex is not None
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if has_recipe_pattern != last_strategy:
                new_strategy = "—à–∏—Ä–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω)" if has_recipe_pattern else "–≥–ª—É–±–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ—Ç–µ—Ä—è–Ω)"
                self.logger.info(f"‚ö° –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {new_strategy}")
                last_strategy = has_recipe_pattern
            
            # DFS: pop() –±–µ—Ä–µ—Ç —Å –∫–æ–Ω—Ü–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π - –ø–µ—Ä–≤—ã–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è)
            # BFS: pop(0) –±–µ—Ä–µ—Ç —Å –Ω–∞—á–∞–ª–∞ (–ø–µ—Ä–≤—ã–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π - –ø–µ—Ä–≤—ã–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è)
            current_url, depth = queue.pop() if not has_recipe_pattern else queue.pop(0)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã
            if depth > max_depth:
                continue
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            pattern = self.get_url_pattern(current_url)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –ø–æ—Å–µ—â–∞—Ç—å
            ignore_visited = len(queue) <= 5  # –í –Ω–∞—á–∞–ª–µ –æ–±—Ö–æ–¥–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ –Ω–∞–±—Ä–∞—Ç—å –±–∞–∑—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            if self.should_explore_url(current_url, ignore_visited=ignore_visited) is False and urls_explored > 0 and not check_pages_with_extractor:
                continue
            
            try:
                self.logger.info(f"[{urls_explored + 1}/{max_urls}] –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞: {current_url}")
                self.logger.info(f"  –ü–∞—Ç—Ç–µ—Ä–Ω: {pattern}, –ì–ª—É–±–∏–Ω–∞: {depth}")
                
                # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∫–∏
                page_load_start = time.time()
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º timeout
                if not self._navigate_with_timeout(current_url, timeout=90):
                    self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    self.failed_urls.add(current_url)
                    err_count += 1
                    continue
                
                # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
                try:
                    # –ñ–¥–µ–º –ª–∏–±–æ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏, –ª–∏–±–æ interactive (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞)
                    WebDriverWait(self.driver, 10).until(
                        lambda d: d.execute_script('return document.readyState') in ['complete', 'interactive']
                    )
                except TimeoutException:
                    self.logger.warning("‚è± Timeout –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å
                    try:
                        body = self.driver.find_element("tag name", "body")
                        if not body:
                            raise RuntimeError("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞—è")
                    except Exception:
                        self.logger.error("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        self.failed_urls.add(current_url)
                        continue
                     
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Cloudflare/Captcha
                try:
                    page_title = self.driver.title.lower()
                    page_source_snippet = self.driver.page_source[:5000].lower()  # –¢–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞—â–∏—Ç—É
                    protection_indicators = [
                        'cloudflare', 'captcha', 'are you a robot', 'access denied',
                        'just a moment', 'challenge', 'verify you are human'
                    ]
                    
                    if any(indicator in page_title or indicator in page_source_snippet 
                           for indicator in protection_indicators):
                        self.logger.warning(f"üõ°Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –±–æ—Ç–æ–≤ –Ω–∞ {current_url}")
                        self.logger.warning("–ü–∞—É–∑–∞ 10 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è...")
                        time.sleep(10)  # –î–∞–µ–º –≤—Ä–µ–º—è —Ä–µ—à–∏—Ç—å –≤—Ä—É—á–Ω—É—é
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑
                        if any(indicator in self.driver.title.lower() for indicator in protection_indicators):
                            self.logger.error("–ó–∞—â–∏—Ç–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º URL")
                            self.failed_urls.add(current_url)
                            err_count += 1
                            continue
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞—â–∏—Ç—ã: {e}")
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏
                total_load_time = time.time() - page_load_start
                self.logger.debug(f"  ‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {total_load_time:.1f}s")
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: –∫–æ—Ä–æ—á–µ –≤ –Ω–∞—á–∞–ª–µ, –¥–ª–∏–Ω–Ω–µ–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 10 –∑–∞–ø—Ä–æ—Å–æ–≤
                self.request_count += 1
                if self.request_count % 10 == 0:
                    # –ö–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ - –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    delay = random.uniform(1, 3)
                    self.logger.info(f"  –î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ {self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤: {delay:.1f}—Å")
                else:
                    # –û–±—ã—á–Ω–∞—è –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
                    delay = random.uniform(0.5, 1)
                time.sleep(delay)
                
                # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
                use_quick_scroll = self.request_count % 3 != 0  # –ö–∞–∂–¥—ã–π 3-–π - –æ–±—ã—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
                self.slow_scroll_page(quick_mode=use_quick_scroll)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ
                self.visited_urls.add(current_url)
                urls_explored += 1
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω
                if pattern not in self.url_patterns:
                    self.url_patterns[pattern] = []
                
                page_index = len(self.url_patterns[pattern]) + 1
                self.url_patterns[pattern].append(current_url)
                
                
                # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω —Ä–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–¥–∞–Ω —Ä–µ–∂–∏–º –ø—Ä–æ–≤–µ–∫—Ä–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
                if check_pages_with_extractor and (check_url is False or self.should_extract_recipe(current_url)):   
                    if self.check_and_extract_recipe(current_url, pattern, page_index):
                        # –ï—Å–ª–∏ URL –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É, –Ω–æ —Ä–µ—Ü–µ–ø—Ç –Ω–∞–π–¥–µ–Ω - –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
                        if self.recipe_regex and not self.is_recipe_url(current_url):
                            self.logger.info("  –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ URL, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–π–¥–µ–Ω —Ä–µ—Ü–µ–ø—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                            if self.analyzer is None:
                                self.analyzer = RecipeAnalyzer()
                            pattern =  self.analyzer.analyse_recipe_page_pattern(site_id=self.site.id)
                            
                # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω regex –ø–∞—Ç—Ç–µ—Ä–Ω - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—Ü–µ–ø—Ç, –∏–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                elif self.should_extract_recipe(current_url):
                    if self.site.pattern: self.mark_page_as_successful(current_url) # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –∑–∞–¥–∞–Ω - –æ—Ç–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π —Ç–∫ –∏–Ω–∞—á–µ –Ω–µ –º–æ–∂–µ–º –∑–Ω–∞—Ç—å –±—ã–ª –ª–∏ –≤–æ–æ–±—â–µ —É—Å–ø–µ—Ö
                    self.save_page_html(current_url, pattern, page_index)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
                new_links = self.extract_links_with_priority()
                self.logger.info(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(new_links)}")
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ –æ—á–µ—Ä–µ–¥—å —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –≥–ª—É–±–∏–Ω—É (DFS)
                # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —à–∏—Ä–∏–Ω—É (BFS)
                has_recipe_pattern = self.recipe_regex is not None
                
                for link_url in new_links:
                    if self.should_explore_url(link_url, ignore_visited=len(queue) <= 5): # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –º–∞–ª–µ–Ω—å–∫–∞—è (–¥–ª—è —Å—Ç–∞—Ä—Ç–∞ –∏ —á—Ç–æ–±—ã –Ω–µ –≤—ã–ª–µ—Ç–µ—Ç—å –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ)
                        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–µ—Ä–µ—Ö–æ–¥–∞
                        if link_url not in self.referrer_map:
                            self.referrer_map[link_url] = current_url
                        
                        # DFS (–≤–≥–ª—É–±—å): –¥–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ –æ—á–µ—Ä–µ–¥–∏ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ—Ç
                        # BFS (–≤—à–∏—Ä—å): –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü –æ—á–µ—Ä–µ–¥–∏ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –µ—Å—Ç—å
                        if has_recipe_pattern and check_url is True:
                            queue.append((link_url, depth + 1))
                        else:
                            queue.insert(0, (link_url, depth + 1))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—á–µ—Ä–µ–¥—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω
                if has_recipe_pattern:
                    queue.sort(key=lambda x: self.get_url_priority(x[0]))
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if urls_explored % 10 == 0:
                    self.exploration_queue = queue  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –æ—á–µ—Ä–µ–¥—å
                    self.save_state()
                
            except KeyboardInterrupt:
                self.logger.warning("‚å®Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ...")
                self.exploration_queue = queue
                self.save_state()
                raise
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {current_url}: {e}")
                self.failed_urls.add(current_url)
                self.exploration_queue = queue  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                self.save_state()  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                err_count += 1
                if err_count >= self.max_errors:
                    self.logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥ ({self.max_errors}), –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.")
                    break
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–µ–π –æ—á–µ—Ä–µ–¥—å—é
        self.exploration_queue = queue
        self.save_state()
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ" if err_count < self.max_errors else "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.save_dir}")
        self.logger.info(f"  - {self.state_file} - —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
        self.logger.info(f"  - {self.patterns_file} - –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        self.logger.info(f"  - *.html - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({sum(len(urls) for urls in self.url_patterns.values())} —Ñ–∞–π–ª–æ–≤)")
        self.logger.info("–î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: explorer.load_state() –∏–ª–∏ explorer.import_state(state)")
        self.logger.info(f"{'='*60}")
        return urls_explored

    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ –ë–î"""
        if self.driver and not self.debug_mode:
            self.driver.quit()
        
        self.site_repository.close()
        self.page_repository.close()
        self.logger.info("–ì–æ—Ç–æ–≤–æ")


def explore_site(url: str, max_urls: int = 1000, max_depth: int = 4, recipe_pattern: str = None,
                 check_pages_with_extractor: bool = False,
                 check_url: bool = False,
                 max_urls_per_pattern: int = None, debug_port: int = 9222,
                 helper_links: List[str] = None,
                 custom_logger: Optional[logging.Logger] = None,
                 max_no_recipe_pages: Optional[int] = None) -> int:
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π
    
    Args:
        url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
        max_urls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        recipe_pattern: Regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ URL —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏
        check_pages_with_extractor: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º —Ä–µ—Ü–µ–ø—Ç–æ–≤
        check_url: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ URL –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—É –ø–µ—Ä–µ–¥ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π
        max_urls_per_pattern: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –Ω–∞ –æ–¥–∏–Ω –ø–∞—Ç—Ç–µ—Ä–Ω (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
        debug_port: –ü–æ—Ä—Ç –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Chrome
        helper_links: –°–ø–∏—Å–æ–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        max_no_recipe_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞ –ø–æ–¥—Ä—è–¥ (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
        custom_logger: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ª–æ–≥–≥–µ—Ä (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
    
    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö URL
    """
    explorer = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ try
    if custom_logger is None:
        custom_logger = logger
    
    try:
        explorer = SiteExplorer(
            url, 
            debug_port=debug_port, 
            debug_mode=True, 
            recipe_pattern=recipe_pattern, 
            max_urls_per_pattern=max_urls_per_pattern,
            custom_logger=custom_logger,
            max_no_recipe_pages=max_no_recipe_pages
        )
        
        if helper_links:
            explorer.add_helper_urls(helper_links, depth=1)
        
        explorer.connect_to_chrome()
        explorer.load_state()
        
        urls_explored = explorer.explore(
            max_urls=max_urls, 
            max_depth=max_depth, 
            check_url=check_url, 
            check_pages_with_extractor=check_pages_with_extractor
        )
        
        custom_logger.info(f"‚úì –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {urls_explored} URL")
        return urls_explored
        
    except KeyboardInterrupt:
        custom_logger.warning("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if explorer:
            explorer.save_state()
        raise
        
    except WebDriverException as e:
        custom_logger.error(f"–û—à–∏–±–∫–∞ WebDriver: {e}")
        if explorer:
            explorer.save_state()
        raise
        
    except Exception as e:
        custom_logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        if explorer:
            explorer.save_state()
        raise
        
    finally:
        if explorer:
            explorer.close()

def run_explorer(explorer:SiteExplorer, max_urls: int, max_depth: int):
    
    try:
        explorer.connect_to_chrome()
        explorer.explore(max_urls=max_urls, max_depth=max_depth)
    except KeyboardInterrupt:
        logger.info("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)
    finally:
        explorer.close()

