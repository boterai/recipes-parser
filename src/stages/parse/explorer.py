"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ –∏ —Å–±–æ—Ä–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
"""
import os
import sys
import time
import json
import re
import random
from pathlib import Path
from urllib.parse import urlparse, urljoin
from typing import Set, Dict, List
import logging

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup

if __name__ == "__main__":
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import config.config as config
from src.common.db.mysql import MySQlManager
from src.stages.extract.recipe_extractor import RecipeExtractor
from src.stages.analyse.analyse import RecipeAnalyzer
import sqlalchemy
from src.models.page import Page
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Multilingual recipe-related keywords for URL and content detection
RECIPE_KEYWORDS = {
    'url': [
        'recipe', 'recipes', 'recette', 'recettes', '—Ä–µ—Ü–µ–ø—Ç', '—Ä–µ—Ü–µ–ø—Ç—ã', 
        'ricetta', 'ricette', 'rezept', 'rezepte', 'receta', 'recetas',
        'tarif', 'tarifler', '„É¨„Ç∑„Éî', 'È£üË∞±', 'Ï°∞Î¶¨Î≤ï', 'ŸàÿµŸÅÿ©'
    ],
    'ingredients': [
        'ingredients', 'ingredient', '–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã', '–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç', 
        'ingr√©dients', 'ingr√©dient', 'ingredientes', 'ingrediente',
        'ingredienti', 'zutaten', 'malzemeler', 'ÊùêÊñô', 'ÈÖçÊñô', 'Ïû¨Î£å'
    ],
    'instructions': [
        'instructions', 'steps', 'directions', 'method', 'preparation',
        '—à–∞–≥–∏', '–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', '√©tapes', 'pr√©paration',
        'paso', 'pasos', 'procedimento', 'zubereitung', 'hazƒ±rlanƒ±≈ü',
        '‰Ωú„ÇäÊñπ', 'ÊâãÈ†Ü', 'Ê≠•È™§', 'Ï°∞Î¶¨ Î∞©Î≤ï'
    ],
    'cooking': [
        'cooking', 'cook', 'cuisine', 'cuire', '–≥–æ—Ç–æ–≤–∏—Ç—å', '–≥–æ—Ç–æ–≤–∫–∞',
        'cocinar', 'cucinare', 'kochen', 'pi≈üirmek', 'ÊñôÁêÜ', 'ÁÉπÈ•™', 'ÏöîÎ¶¨'
    ],
    'time': [
        'cooking time', 'prep time', '–≤—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è', '–≤—Ä–µ–º—è –≥–æ—Ç–æ–≤–∫–∏',
        'temps de pr√©paration', 'tiempo de preparaci√≥n', 'tempo di preparazione',
        'vorbereitungszeit', 'hazƒ±rlama s√ºresi', 'Ë™øÁêÜÊôÇÈñì', 'ÂáÜÂ§áÊó∂Èó¥', 'Ï°∞Î¶¨ ÏãúÍ∞Ñ'
    ],
    'dish_types': [
        'dinner', 'lunch', 'breakfast', 'dessert', 'appetizer', 'snack',
        '–æ–±–µ–¥', '—É–∂–∏–Ω', '–∑–∞–≤—Ç—Ä–∞–∫', '–¥–µ—Å–µ—Ä—Ç', '–∑–∞–∫—É—Å–∫–∞',
        'd√Æner', 'd√©jeuner', 'petit-d√©jeuner', 'dessert', 'entr√©e',
        'cena', 'comida', 'desayuno', 'postre', 'aperitivo',
        'abendessen', 'mittagessen', 'fr√ºhst√ºck', 'nachtisch',
        'ak≈üam yemeƒüi', '√∂ƒüle yemeƒüi', 'kahvaltƒ±', 'tatlƒ±'
    ],
    'common_foods': [
        'chicken', 'fish', 'beef', 'pork', 'pasta', 'rice', 'salad', 'soup',
        '–∫—É—Ä–∏—Ü–∞', '—Ä—ã–±–∞', '–≥–æ–≤—è–¥–∏–Ω–∞', '—Å–≤–∏–Ω–∏–Ω–∞', '–ø–∞—Å—Ç–∞', '—Ä–∏—Å', '—Å–∞–ª–∞—Ç', '—Å—É–ø',
        'poulet', 'poisson', 'boeuf', 'porc', 'p√¢tes', 'riz', 'salade', 'soupe',
        'pollo', 'pescado', 'carne', 'cerdo', 'arroz', 'ensalada', 'sopa',
        'tavuk', 'balƒ±k', 'et', 'makarna', 'pilav', 'salata', '√ßorba'
    ]
}


class SiteExplorer:
    """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤"""
    
    def __init__(self, base_url: str, debug_mode: bool = True, use_db: bool = True, recipe_pattern: str = None,
                 max_errors: int = 3):
        """
        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
            debug_mode: –ï—Å–ª–∏ True, –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –æ—Ç–∫—Ä—ã—Ç–æ–º—É Chrome —Å –æ—Ç–ª–∞–¥–∫–æ–π
            use_db: –ï—Å–ª–∏ True, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ MySQL
            recipe_pattern: Regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ URL —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_errors: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥ –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
        """
        self.base_url = base_url
        self.debug_mode = debug_mode
        self.use_db = use_db
        self.driver = None
        self.db = None
        self.site_id = None
        self.recipe_pattern = recipe_pattern
        self.recipe_regex = None
        self.request_count = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
        self.max_errors = max_errors
        self.analyzer = None
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è regex –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if recipe_pattern:
            try:
                self.recipe_regex = re.compile(recipe_pattern)
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–µ—Ü–µ–ø—Ç–æ–≤: {recipe_pattern}")
            except re.error as e:
                logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω: {e}")
                self.recipe_regex = None
        
        parsed_url = urlparse(base_url)
        self.base_domain = parsed_url.netloc.replace('www.', '')
        self.site_name = self.base_domain.replace('.', '_')
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}/"
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        self.visited_urls: Set[str] = set()
        self.url_patterns: Dict[str, List[str]] = {}  # –ø–∞—Ç—Ç–µ—Ä–Ω -> —Å–ø–∏—Å–æ–∫ URL
        self.failed_urls: Set[str] = set()
        self.referrer_map: Dict[str, str] = {}  # URL -> referrer URL (–æ—Ç–∫—É–¥–∞ –ø—Ä–∏—à–ª–∏)
        self.successful_referrers: Set[str] = set()  # URLs —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Ä–µ—Ü–µ–ø—Ç–∞–º
        self.exploration_queue: List[tuple] = []  # –û—á–µ—Ä–µ–¥—å URL –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: [(url, depth), ...]
        
        # –§–∞–π–ª—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.save_dir = os.path.join(config.PARSED_DIR, self.site_name,"exploration")
        os.makedirs(self.save_dir, exist_ok=True)
        
        self.state_file = os.path.join(self.save_dir, "exploration_state.json")
        self.patterns_file = os.path.join(self.save_dir, "url_patterns.json")
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        if self.use_db:
            self.db = MySQlManager()
            if self.db.connect():
                self.site_id = self.db.create_or_get_site(
                    name=self.site_name,
                    base_url=base_url,
                    language=None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ
                )
                if self.site_id:
                    logger.info(f"–†–∞–±–æ—Ç–∞ —Å —Å–∞–π—Ç–æ–º ID: {self.site_id}")
                    
                    # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –∑–∞–¥–∞–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ë–î
                    if not recipe_pattern:
                        self.load_pattern_from_db()
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL –∏–∑ –ë–î
                    self.load_visited_urls_from_db()
                else:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å/–ø–æ–ª—É—á–∏—Ç—å ID —Å–∞–π—Ç–∞")
                    self.use_db = False
            else:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ë–î")
                self.use_db = False
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ—Ü–µ–ø—Ç–æ–≤
        self.recipe_extractor = None
        if self.use_db and self.db:
            self.recipe_extractor = RecipeExtractor(self.db)


    def set_pattern(self, pattern: str):
        self.recipe_pattern = pattern
        try:
            self.recipe_regex = re.compile(pattern)
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–µ—Ü–µ–ø—Ç–æ–≤: {pattern}")
        except re.error as e:
            logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω: {e}")
            self.recipe_regex = None
    
    def load_pattern_from_db(self):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ regex –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤ –∏–∑ –ë–î –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞
        """
        if not self.use_db or not self.site_id:
            return
        
        try:
            session = self.db.get_session()
            
            sql = "SELECT recipe_pattern FROM sites WHERE id = :site_id"
            result = session.execute(sqlalchemy.text(sql), {"site_id": self.site_id})
            row = result.fetchone()
            
            if row and row[0]:
                pattern = row[0]
                self.recipe_pattern = pattern
                try:
                    self.recipe_regex = re.compile(pattern)
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –ë–î: {pattern}")
                except re.error as e:
                    logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –ë–î: {e}")
                    self.recipe_regex = None
            else:
                logger.info("–ü–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î")
            
            session.close()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–∑ –ë–î: {e}")
    
    def load_visited_urls_from_db(self):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –∏–∑ –ë–î
        """
        if not self.use_db or not self.site_id:
            return
        
        try:
            session = self.db.get_session()
            
            sql = "SELECT url, pattern FROM pages WHERE site_id = :site_id"
            result = session.execute(sqlalchemy.text(sql), {"site_id": self.site_id})
            rows = result.fetchall()
            
            loaded_count = 0
            for url, pattern in rows:
                if url:
                    self.visited_urls.add(url)
                    loaded_count += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                    if pattern:
                        if pattern not in self.url_patterns:
                            self.url_patterns[pattern] = []
                        if url not in self.url_patterns[pattern]:
                            self.url_patterns[pattern].append(url)
            
            if loaded_count > 0:
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –∏–∑ –ë–î")
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(self.url_patterns)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            else:
                logger.info("–í –ë–î –Ω–µ—Ç —Ä–∞–Ω–µ–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –¥–ª—è —ç—Ç–æ–≥–æ —Å–∞–π—Ç–∞")
            
            session.close()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –∏–∑ –ë–î: {e}")
    
    def connect_to_chrome(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ"""
        chrome_options = Options()
        
        if self.debug_mode:
            chrome_options.add_experimental_option(
                "debuggerAddress", 
                f"localhost:{config.CHROME_DEBUG_PORT}"
            )
            logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Chrome –Ω–∞ –ø–æ—Ä—Ç—É {config.CHROME_DEBUG_PORT}")
        else:
            chrome_options.add_argument("--start-maximized")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            # –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –º–µ–Ω—å—à–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(config.IMPLICIT_WAIT)
            self.driver.set_page_load_timeout(config.PAGE_LOAD_TIMEOUT)
            logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±—Ä–∞—É–∑–µ—Ä—É")
        except WebDriverException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±—Ä–∞—É–∑–µ—Ä—É: {e}")
            if self.debug_mode:
                logger.error(
                    f"\n–ó–∞–ø—É—Å—Ç–∏—Ç–µ Chrome –∫–æ–º–∞–Ω–¥–æ–π:\n"
                    f"google-chrome --remote-debugging-port={config.CHROME_DEBUG_PORT} "
                    f"--user-data-dir=./chrome_debug\n"
                )
            raise
    
    def get_url_pattern(self, url: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ URL –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Å—Å—ã–ª–æ–∫
        
        Args:
            url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ü–∞—Ç—Ç–µ—Ä–Ω URL (—á–∏—Å–ª–∞ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ #, id –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ {id})
        """
        parsed = urlparse(url)
        path = parsed.path
        
        # –ó–∞–º–µ–Ω–∞ —á–∏—Å–µ–ª –Ω–∞ #
        pattern = re.sub(r'\d+', '#', path)
        
        # –ó–∞–º–µ–Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ {id}
        pattern = re.sub(r'[a-f0-9]{8,}', '{id}', pattern, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ trailing slash –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
        pattern = pattern.rstrip('/')
        
        return pattern or '/'
    
    def is_same_domain(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ URL —Ç–æ–º—É –∂–µ –¥–æ–º–µ–Ω—É"""
        try:
            domain = urlparse(url).netloc.replace('www.', '')
            return domain == self.base_domain
        except Exception:
            return False
    
    def is_recipe_url(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ URL —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
        """
        if not self.recipe_regex:
            return False
        
        try:
            parsed = urlparse(url)
            path = parsed.path
            return len(re.findall(self.recipe_pattern, path)) > 0
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ URL {url}: {e}")
            return False
    
    def check_and_extract_recipe(self, url: str, pattern: str, page_index: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ä–µ—Ü–µ–ø—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î
        
        Args:
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            save_html: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ HTML —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
            
        Returns:
            (is_recipe, confidence_score, recipe_data)
            - is_recipe: True –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ä–µ—Ü–µ–ø—Ç
            - confidence_score: —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0-100)
            - recipe_data: –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–∞ –∏–ª–∏ None
        """
        if not (self.use_db and self.site_id):
            logger.warning(" –ë–î –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–µ—Ü–µ–ø—Ç–∞")
            return False
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Page –¥–ª—è –ë–î


        page = Page(site_id=self.site_id, 
                    url=url, 
                    pattern=pattern, 
                    html_path=self.save_page_as_file(pattern, page_index))


        
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–∞
        recipe_data = self.recipe_extractor.extract_and_update_page(page)
        
        # UPSERT
        upsert_sql = """
            INSERT INTO pages (
                site_id, url, pattern, html_path,
                is_recipe, confidence_score,
                dish_name, description, 
                ingredient, step_by_step,
                prep_time, cook_time, total_time,
                servings, difficulty_level,
                category, nutrition_info,
                notes, rating, tags, title, language, image_urls
            ) VALUES (
                :site_id, :url, :pattern, :html_path,
                :is_recipe, :confidence_score,
                :dish_name, :description,
                :ingredient, :step_by_step,
                :prep_time, :cook_time, :total_time,
                :servings, :difficulty_level,
                :category, :nutrition_info,
                :notes, :rating, :tags, :title, :language, :image_urls
            )
            ON DUPLICATE KEY UPDATE
                is_recipe = VALUES(is_recipe),
                confidence_score = VALUES(confidence_score),
                dish_name = VALUES(dish_name),
                description = VALUES(description),
                ingredient = VALUES(ingredient),
                step_by_step = VALUES(step_by_step),
                prep_time = VALUES(prep_time),
                cook_time = VALUES(cook_time),
                total_time = VALUES(total_time),
                servings = VALUES(servings),
                difficulty_level = VALUES(difficulty_level),
                category = VALUES(category),
                nutrition_info = VALUES(nutrition_info),
                notes = VALUES(notes),
                rating = VALUES(rating),
                tags = VALUES(tags),
                title = VALUES(title),
                language = VALUES(language),
                image_urls = VALUES(image_urls)
        """

        upsert_on_non_recipe = """
            INSERT INTO pages (
                site_id, url, pattern, html_path,
                is_recipe, confidence_score, title, language
            ) VALUES (
                :site_id, :url, :pattern, :html_path,
                :is_recipe, :confidence_score, :title, :language
            )
            ON DUPLICATE KEY UPDATE
                is_recipe = VALUES(is_recipe),
                confidence_score = VALUES(confidence_score), 
                title = VALUES(title),
                language = VALUES(language)
            """
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        upsert_data = {
            "site_id": self.site_id,
            "url": url,
            "pattern": pattern,
            "html_path": page.html_path,
            "title": self.driver.title,
            "language": self.driver.execute_script("return document.documentElement.lang") or 'unknown',
            **recipe_data
        }

        if recipe_data.get("is_recipe", False) is True:
            self.mark_page_as_successful(url)
        else:
            upsert_sql = upsert_on_non_recipe

        try:
            with self.db.get_session() as session:
                session.execute(sqlalchemy.text(upsert_sql), upsert_data)
                session.commit()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ë–î: {e}")
            return False
        
        dish_name = recipe_data.get('dish_name', 'Unknown')
        logger.info(f"  ‚úì –†–µ—Ü–µ–ø—Ç '{dish_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î")
        return True
        

    def get_recipe_likelihood_score(self, url: str, link_text: str = "", context_text: str = "") -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ URL –≤–µ–¥–µ—Ç –∫ —Ä–µ—Ü–µ–ø—Ç—É (0-100)
        
        Args:
            url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            link_text: –¢–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ (anchor text)
            context_text: –û–∫—Ä—É–∂–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–∫–∏
            
        Returns:
            –û—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç 0 –¥–æ 100
        """
        score = 0.0
        url_lower = url.lower()
        link_text_lower = link_text.lower()
        context_lower = context_text.lower()
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ URL (–º–∞–∫—Å–∏–º—É–º 40 –±–∞–ª–ª–æ–≤)
        # –ü—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å recipe keywords –≤ URL
        url_recipe_matches = sum(1 for kw in RECIPE_KEYWORDS['url'] if kw in url_lower)
        score += min(url_recipe_matches * 15, 40)  # –î–æ 40 –±–∞–ª–ª–æ–≤
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã URL —Å –Ω–æ–º–µ—Ä–∞–º–∏ (—á–∞—Å—Ç–æ —Ä–µ—Ü–µ–ø—Ç—ã)
        if re.search(r'/\d{4,}', url) or re.search(r'recipe[-_]\d+', url_lower):
            score += 10
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏ (–º–∞–∫—Å–∏–º—É–º 30 –±–∞–ª–ª–æ–≤)
        # –ü—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å keywords –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏
        for category, keywords in RECIPE_KEYWORDS.items():
            matches = sum(1 for kw in keywords if kw in link_text_lower)
            if category == 'url':
                score += min(matches * 10, 20)
            elif category in ['ingredients', 'instructions']:
                score += min(matches * 5, 10)
        
        # –ù–∞–∑–≤–∞–Ω–∏—è –±–ª—é–¥ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏
        dish_matches = sum(1 for kw in RECIPE_KEYWORDS['common_foods'] if kw in link_text_lower)
        score += min(dish_matches * 3, 10)
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–∞–∫—Å–∏–º—É–º 20 –±–∞–ª–ª–æ–≤)
        context_recipe_score = 0
        for category in ['ingredients', 'instructions', 'cooking', 'time']:
            matches = sum(1 for kw in RECIPE_KEYWORDS[category] if kw in context_lower)
            context_recipe_score += matches
        score += min(context_recipe_score * 2, 20)
        
        # 4. –ë–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–º–∞–∫—Å–∏–º—É–º 10 –±–∞–ª–ª–æ–≤)
        # URL —Å–æ–¥–µ—Ä–∂–∏—Ç recipe + —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –µ–¥—É
        if any(kw in url_lower for kw in RECIPE_KEYWORDS['url']):
            if any(kw in link_text_lower for kw in RECIPE_KEYWORDS['common_foods']):
                score += 10
        
        return min(score, 100)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º–æ–º 100
    

    def quick_recipe_check(self, soup: BeautifulSoup = None) -> tuple:
        """
        –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–µ—Ü–µ–ø—Ç–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç (–µ—Å–ª–∏ None, –ø–∞—Ä—Å–∏—Ç —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É)
            
        Returns:
            (has_recipe, confidence): True –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ —Ä–µ—Ü–µ–ø—Ç, –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ 0-100
        """
        if soup is None:
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
        
        confidence = 0
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ JSON-LD schema (30 –±–∞–ª–ª–æ–≤)
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    schema_type = data.get('@type', '')
                    if 'Recipe' in str(schema_type):
                        confidence += 30
                        break
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and 'Recipe' in str(item.get('@type', '')):
                            confidence += 30
                            break
            except (json.JSONDecodeError, AttributeError):
                continue
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ meta —Ç–µ–≥–æ–≤ (20 –±–∞–ª–ª–æ–≤)
        og_type = soup.find('meta', property='og:type')
        if og_type and 'recipe' in og_type.get('content', '').lower():
            confidence += 20
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤ (20 –±–∞–ª–ª–æ–≤)
        recipe_indicators = [
            soup.find('div', class_=re.compile(r'recipe', re.I)),
            soup.find('article', class_=re.compile(r'recipe', re.I)),
            soup.find(attrs={'itemtype': re.compile(r'Recipe', re.I)}),
        ]
        if any(recipe_indicators):
            confidence += 20
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (30 –±–∞–ª–ª–æ–≤)
        text = soup.get_text().lower()
        
        # –ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã
        has_ingredients = any(kw in text for kw in RECIPE_KEYWORDS['ingredients'][:5])
        if has_ingredients:
            confidence += 10
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        has_instructions = any(kw in text for kw in RECIPE_KEYWORDS['instructions'][:5])
        if has_instructions:
            confidence += 10
        
        # –í—Ä–µ–º—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è
        has_time = any(kw in text for kw in RECIPE_KEYWORDS['time'][:5])
        if has_time:
            confidence += 5
        
        # –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
        food_count = sum(1 for kw in RECIPE_KEYWORDS['common_foods'][:20] if kw in text)
        confidence += min(food_count, 5)
        
        # –†–µ—à–µ–Ω–∏–µ: —Ä–µ—Ü–µ–ø—Ç –µ—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å >= 40
        return (confidence >= 40, confidence)
    
    
    def should_explore_url(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π URL
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏            
        Returns:
            True –µ—Å–ª–∏ URL –Ω—É–∂–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç—å
        """
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –ø–æ—Å–µ—â–∞–ª–∏
        if url in self.visited_urls:
            return False
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã
        if re.search(r'\.(jpg|jpeg|png|gif|pdf|zip|mp4|avi|css|js)$', url, re.IGNORECASE):
            return False
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        skip_patterns = [
            r'/answers'
            r'/login',
            r'/register',
            r'/signup',
            r'/blog',
            r'/news',
            r'/forum',
            r'/admin',
            r'/dashboard',
            r'/logout',
            r'/user'
            r'/signin',
            r'/auth',
            r'/account',
            r'/profile',
            r'/settings',
            r'/about',
            r'/contact',
            r'/privacy',
            r'/terms',
            r'/cookie',
            r'/newsletter',
            r'/subscribe',
            r'/unsubscribe',
            r'/cart',
            r'/checkout',
            r'/order',
            r'/search',
            r'/feedback',
            r'/help',
            r'/support',
            r'/faq',
            r'/advertise',
            r'/careers',
            r'/jobs',
        ]
        
        parsed = urlparse(url)
        path_lower = parsed.path.lower()
        
        for skip_pattern in skip_patterns:
            if re.search(skip_pattern, path_lower):
                logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ —Å–ª—É–∂–µ–±–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
                return False
        return True
    
    def get_url_priority(self, url: str) -> int:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ URL –¥–ª—è –æ–±—Ö–æ–¥–∞
        TODO –ø–æ—É–¥–º–∞—Ç—å –Ω–∞–¥ –ª–æ–≥–∏–∫–æ–π –∫–∞–∫-—Ç–æ —Ç—É—Ç –Ω–µ –ø—Ä—è–º —Å—É–ø–µ—Ä
        Args:
            url: URL –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–º–µ–Ω—å—à–µ = –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0 (–Ω–∞–∏–≤—ã—Å—à–∏–π): URL —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º —Ä–µ—Ü–µ–ø—Ç–∞
        if self.is_recipe_url(url):
            return 0
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: URL —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Ä–µ—Ü–µ–ø—Ç–∞–º
        referrer = self.referrer_map.get(url)
        if referrer and referrer in self.successful_referrers:
            return 1
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –æ—Å—Ç–∞–ª—å–Ω—ã–µ URL
        return 2
    
    def slow_scroll_page(self, quick_mode: bool = False):
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            quick_mode: –ï—Å–ª–∏ True, –¥–µ–ª–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–∫—Ä—É—Ç–∫—É (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        """
        try:
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if quick_mode:
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞: 2-3 —à–∞–≥–∞ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø–∞—É–∑–∞–º–∏
                num_scrolls = random.randint(2, 3)
                scroll_step = total_height // num_scrolls
                
                current_position = 0
                for i in range(num_scrolls):
                    current_position += scroll_step
                    self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                    time.sleep(random.uniform(0.3, 0.5))
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –≤ –∫–æ–Ω–µ—Ü
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(0.3, 0.6))
            else:
                # –û–±—ã—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
                num_scrolls = random.randint(3, 5)
                scroll_step = total_height // num_scrolls
                
                current_position = 0
                for i in range(num_scrolls):
                    current_position += scroll_step
                    self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                    time.sleep(random.uniform(0.4, 0.8))
                
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(0.5, 1.0))
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–∫—Ä—É—Ç–∫–µ: {e}")


    def save_page_as_file(self, pattern: str, page_index: int) -> str:
        """        
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
        Args:
            pattern: –ü–∞—Ç—Ç–µ—Ä–Ω URL
            page_index: –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É HTML
        """

        html_content = self.driver.page_source
            
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        safe_pattern = pattern.replace('/', '_').replace('#', 'N').replace('{', '').replace('}', '').strip('_')
        if not safe_pattern:
            safe_pattern = 'index'
        
        filename = f"{safe_pattern}_{page_index}.html"
        filepath = os.path.join(self.save_dir,filename)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return filepath

    
    def save_page_html(self, url: str, pattern: str, page_index: int):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ë–î
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            pattern: –ü–∞—Ç—Ç–µ—Ä–Ω URL
            page_index: –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        """
        try:
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            title = self.driver.title
            language = self.driver.execute_script("return document.documentElement.lang") or 'unknown'
            filepath = self.save_page_as_file(pattern, page_index)
            filename = os.path.basename(filepath)
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
            if self.use_db and self.site_id:
                page_id = self.db.save_page(
                    site_id=self.site_id,
                    url=url,
                    pattern=pattern,
                    title=title,
                    language=language,
                    html_path=os.path.relpath(filepath),
                )
                if page_id:
                    logger.info(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} (DB ID: {page_id})")
                else:
                    logger.info(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} (–ë–î: –æ—à–∏–±–∫–∞)")
            else:
                logger.info(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
    
    def extract_links(self) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏)
        –î–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ extract_links_with_priority()
        """
        try:
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(self.driver.current_url, href)
                
                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç —è–∫–æ—Ä–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                clean_url = absolute_url.split('#')[0].split('?')[0]
                
                if clean_url and self.is_same_domain(clean_url):
                    links.append(clean_url)
            
            return list(set(links))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")
            return []
    

    def extract_links_with_priority(self) -> List[tuple]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–µ—Ü–µ–ø—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (url, likelihood_score, link_text) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        """
        try:
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            links_with_scores = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(self.driver.current_url, href)
                
                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç —è–∫–æ—Ä–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                clean_url = absolute_url.split('#')[0].split('?')[0]
                
                if not (clean_url and self.is_same_domain(clean_url)):
                    continue
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
                link_text = link.get_text(strip=True)
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Ä–æ–¥–∏—Ç–µ–ª—å + —Å–æ—Å–µ–¥–∏)
                context_parts = []
                
                # –¢–µ–∫—Å—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                parent = link.parent
                if parent:
                    # –¢–µ–∫—Å—Ç –¥–æ —Å—Å—ã–ª–∫–∏
                    for sibling in parent.find_all_previous(string=True, limit=3):
                        if sibling.strip():
                            context_parts.insert(0, sibling.strip())
                    
                    # –¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Å—Å—ã–ª–∫–∏
                    for sibling in parent.find_all_next(string=True, limit=3):
                        if sibling.strip():
                            context_parts.append(sibling.strip())
                
                context_text = ' '.join(context_parts)[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                
                # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                score = self.get_recipe_likelihood_score(clean_url, link_text, context_text)
                
                links_with_scores.append((clean_url, score, link_text))
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score)
            unique_links = {}
            for url, score, text in links_with_scores:
                if url not in unique_links or score > unique_links[url][0]:
                    unique_links[url] = (score, text)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
            result = [(url, score, text) for url, (score, text) in unique_links.items()]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            result.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
            if result:
                top_5 = result[:5]
                logger.info("–¢–æ–ø-5 —Å—Å—ã–ª–æ–∫ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
                for url, score, text in top_5:
                    logger.info(f"  [{score:.0f}] {text[:30]}... -> {url[:60]}...")
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: {e}")
            return []
    
    
    def  export_state(self) -> dict:
        """–≠–∫—Å–ø–æ—Ä—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –¥—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º explorer
        """
        return {
            'base_url': self.base_url,
            'recipe_pattern': self.recipe_pattern,
            'visited_urls': list(self.visited_urls),
            'url_patterns': dict(self.url_patterns),
            'failed_urls': list(self.failed_urls),
            'referrer_map': dict(self.referrer_map),
            'successful_referrers': list(self.successful_referrers),
            'exploration_queue': list(self.exploration_queue),
            'request_count': self.request_count,
            'site_id': self.site_id,
            'site_name': self.site_name,
            'exported_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def add_helper_urls(self, urls: List[str], depth: int = 0):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ URL –≤ –æ—á–µ—Ä–µ–¥—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            depth: –ù–∞—á–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–ª—è —ç—Ç–∏—Ö URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0)
        """
        added_count = 0
        for url in urls:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞
            if not self.is_same_domain(url):
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω URL –¥—Ä—É–≥–æ–≥–æ –¥–æ–º–µ–Ω–∞: {url}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –µ—â–µ –Ω–µ –ø–æ—Å–µ—â–µ–Ω –∏ –Ω–µ –≤ –æ—á–µ—Ä–µ–¥–∏
            if url not in self.visited_urls and (url, depth) not in self.exploration_queue:
                self.exploration_queue.append((url, depth))
                added_count += 1
                logger.info(f"  + –î–æ–±–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å: {url}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—á–µ—Ä–µ–¥—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        self.exploration_queue.sort(key=lambda x: self.get_url_priority(x[0]))
        
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö URL –≤ –æ—á–µ—Ä–µ–¥—å")
        logger.info(f"–í—Å–µ–≥–æ –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.exploration_queue)} URL")
    
    def import_state(self, state: dict):
        """–ò–º–ø–æ—Ä—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ –¥—Ä—É–≥–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        
        Args:
            state: –°–ª–æ–≤–∞—Ä—å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ export_state()
        """
        self.visited_urls = set(state.get('visited_urls', []))
        self.url_patterns = {k: v for k, v in state.get('url_patterns', {}).items()}
        self.failed_urls = set(state.get('failed_urls', []))
        self.referrer_map = dict(state.get('referrer_map', {}))
        self.successful_referrers = set(state.get('successful_referrers', []))
        self.exploration_queue = [tuple(item) for item in state.get('exploration_queue', [])]
        self.request_count = state.get('request_count', 0)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º regex –ø–∞—Ç—Ç–µ—Ä–Ω –µ—Å–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è
        new_pattern = state.get('recipe_pattern')
        if new_pattern and new_pattern != self.recipe_pattern:
            self.recipe_pattern = new_pattern
            try:
                self.recipe_regex = re.compile(new_pattern)
                logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω regex –ø–∞—Ç—Ç–µ—Ä–Ω: {new_pattern}")
            except re.error as e:
                logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π regex –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ: {e}")
        
        logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL, "
                   f"{len(self.url_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, {len(self.exploration_queue)} URL –≤ –æ—á–µ—Ä–µ–¥–∏, "
                   f"{self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        state = self.export_state()
        
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        patterns_data = {
            'patterns': dict(self.url_patterns),
            'total_patterns': len(self.url_patterns),
            'total_unique_urls': sum(len(urls) for urls in self.url_patterns.values())
        }
        
        with open(self.patterns_file, 'w', encoding='utf-8') as f:
            json.dump(patterns_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–æ, {len(self.url_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    
    def load_state(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.state_file):
            logger.info("–§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
            return False
        
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)
            
            self.import_state(state)
            
            logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
            logger.info(f"  –ü–æ—Å–µ—â–µ–Ω–æ URL: {len(self.visited_urls)}")
            logger.info(f"  –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.url_patterns)}")
            logger.info(f"  URL –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.exploration_queue)}")
            logger.info(f"  –£—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.successful_referrers)}")
            logger.info(f"  –û—à–∏–±–æ–∫: {len(self.failed_urls)}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            return False
        
    
    def should_extract_recipe(self, current_url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            current_url: URL —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            check_url: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É —Ä–µ—Ü–µ–ø—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç
        """
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –∑–∞–¥–∞–Ω - –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ–≥–¥–∞
        if self.recipe_regex is None:
            return True
        
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –∑–∞–¥–∞–Ω - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        return self.is_recipe_url(current_url)
        
    
    def mark_page_as_successful(self, current_url: str):
        """
        –û—Ç–º–µ—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ —É—Å–ø–µ—à–Ω—É—é (—Å —Ä–µ—Ü–µ–ø—Ç–æ–º) –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        """
        referrer = self.referrer_map.get(current_url)
        if referrer:
            self.successful_referrers.add(referrer)
            logger.info(f"  ‚úì –ò—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–º–µ—á–µ–Ω –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π: {referrer}")
    

    def explore_multilingual(self, max_urls: int = 100, max_depth: int = 3, 
                            min_likelihood: float = 30.0, quick_check: bool = True) -> int:
        """
        –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π —Ä–µ—Ü–µ–ø—Ç–æ–≤
        
        Args:
            max_urls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
            min_likelihood: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è URL (0-100)
            quick_check: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —Ä–µ—Ü–µ–ø—Ç –ø–µ—Ä–µ–¥ –ø–æ–ª–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤
        """
        logger.info(f"üåç –ù–∞—á–∞–ª–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞: {self.base_url}")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_urls={max_urls}, max_depth={max_depth}, min_likelihood={min_likelihood}")
        
        # –û—á–µ—Ä–µ–¥—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏: (priority_score, url, depth, link_text)
        import heapq
        priority_queue = []
        
        # –°—Ç–∞—Ä—Ç—É–µ–º —Å –±–∞–∑–æ–≤–æ–≥–æ URL
        heapq.heappush(priority_queue, (-100, self.base_url, 0, "Home"))
        
        urls_explored = 0
        recipes_found = 0
        
        while priority_queue and urls_explored < max_urls:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º (heapq - min-heap, –ø–æ—ç—Ç–æ–º—É –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)
            neg_priority, current_url, depth, link_text = heapq.heappop(priority_queue)
            priority = -neg_priority
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã
            if depth > max_depth:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –ø–æ—Å–µ—â–∞—Ç—å
            if current_url in self.visited_urls:
                continue
            
            if not self.should_explore_url(current_url):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º URL —Å –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è)
            if depth > 0 and priority < min_likelihood:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω URL —Å –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é [{priority:.0f}]: {current_url}")
                continue
            
            try:
                logger.info(f"[{urls_explored + 1}/{max_urls}] [{priority:.0f}] {link_text[:30]}...")
                logger.info(f"  URL: {current_url}")
                logger.info(f"  –ì–ª—É–±–∏–Ω–∞: {depth}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                try:
                    self.driver.get(current_url)
                except TimeoutException:
                    logger.warning(f"Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {current_url}")
                    continue
                
                # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
                try:
                    WebDriverWait(self.driver, 15).until(
                        lambda d: d.execute_script('return document.readyState') == 'complete'
                    )
                except TimeoutException:
                    logger.warning("Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                self.request_count += 1
                delay = random.uniform(1.0, 2.0) if self.request_count % 10 != 0 else random.uniform(3, 5)
                time.sleep(delay)
                
                # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                self.slow_scroll_page(quick_mode=True)
                
                # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–π
                self.visited_urls.add(current_url)
                urls_explored += 1
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                pattern = self.get_url_pattern(current_url)
                if pattern not in self.url_patterns:
                    self.url_patterns[pattern] = []
                
                page_index = len(self.url_patterns[pattern]) + 1
                self.url_patterns[pattern].append(current_url)
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ—Ü–µ–ø—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
                is_recipe = False
                confidence = 0
                
                if quick_check:
                    is_recipe, confidence = self.quick_recipe_check()
                    logger.info(f"  –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {'‚úì –†–ï–¶–ï–ü–¢' if is_recipe else '‚úó –Ω–µ —Ä–µ—Ü–µ–ø—Ç'} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence})")
                
                # –ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–µ—Ü–µ–ø—Ç - –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω—É—é —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—é
                if is_recipe and self.recipe_extractor:
                    if self.check_and_extract_recipe(current_url, pattern, page_index):
                        recipes_found += 1
                        logger.info(f"  üéØ –ù–∞–π–¥–µ–Ω–æ —Ä–µ—Ü–µ–ø—Ç–æ–≤: {recipes_found}")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if self.recipe_regex is None or not self.is_recipe_url(current_url):
                            logger.info("  üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤...")
                            if self.analyzer is None:
                                self.analyzer = RecipeAnalyzer(
                                    site_id=self.site_id,
                                    db_manager=self.db,
                                    sample_size=10
                                )
                            new_pattern = self.analyzer.analyse_recipe_page_pattern(site_id=self.site_id)
                            if new_pattern:
                                self.set_pattern(new_pattern)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
                new_links = self.extract_links_with_priority()
                logger.info(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(new_links)}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—É—é –æ—á–µ—Ä–µ–¥—å
                added = 0
                for link_url, link_score, link_txt in new_links:
                    if link_url not in self.visited_urls and link_score >= min_likelihood:
                        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                        if link_url not in self.referrer_map:
                            self.referrer_map[link_url] = current_url
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è max-heap)
                        heapq.heappush(priority_queue, (-link_score, link_url, depth + 1, link_txt))
                        added += 1
                
                logger.info(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {added} —Å—Å—ã–ª–æ–∫ (–º–∏–Ω. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å {min_likelihood})")
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if urls_explored % 10 == 0:
                    self.save_state()
                    logger.info(f"üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {urls_explored} URL, {recipes_found} —Ä–µ—Ü–µ–ø—Ç–æ–≤")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {current_url}: {e}")
                self.failed_urls.add(current_url)
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_state()
        
        logger.info("\n" + "="*60)
        logger.info("üéâ –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"–ü–æ—Å–µ—â–µ–Ω–æ URL: {urls_explored}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ—Ü–µ–ø—Ç–æ–≤: {recipes_found}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.url_patterns)}")
        logger.info(f"–£—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.successful_referrers)}")
        logger.info(f"–û—à–∏–±–æ–∫: {len(self.failed_urls)}")
        logger.info("="*60 + "\n")
        
        return recipes_found
    
    
    def explore(self, max_urls: int = 100, max_depth: int = 3, session_urls: bool = True, 
                check_pages_with_extractor:bool = False,
                forbid_success_mark: bool = False,
                check_url: bool = False) -> int:
        """
        –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
        
        Args:
            max_urls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
            session_urls: –ï—Å–ª–∏ True, —Ç–æ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL –ø—Ä–∏ –ø–æ–¥—Å—á—Ç–µ–µ max urls
            forbid_success_mark: –ï—Å–ª–∏ True, –Ω–µ –æ—Ç–º–µ—á–∞–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è —Å–ª—É—á–∞–µ–≤ –æ—Ç—Å—É—Ç—Å–≤–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞)
            check_pages_with_extractor: –ï—Å–ª–∏ True, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º —Ä–µ—Ü–µ–ø—Ç–æ–≤
            check_url: –ï—Å–ª–∏ True, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—ã–π –Ω–∞ —Ä–µ–¥–∂–µ–∫—Å –ø–∞—Ç—Ç–µ—Ä–Ω –ø–µ—Ä–µ–¥ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π (–ø–∞—Ä–∞–º—Ç–µ—Ä –∫–∞—Å–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏)
        Returns:
            urls_explored: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        """
        logger.info(f"–ù–∞—á–∞–ª–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞: {self.base_url}")
        logger.info(f"–¶–µ–ª—å: –Ω–∞–π—Ç–∏ –¥–æ {max_urls} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ URL")
        
        # –û—á–µ—Ä–µ–¥—å URL –¥–ª—è –æ–±—Ö–æ–¥–∞: (url, depth)
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –æ—á–µ—Ä–µ–¥—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ –Ω–∞—á–∏–Ω–∞–µ–º —Å base_url
        if self.exploration_queue:
            queue = list(self.exploration_queue)
            logger.info(f"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –æ—á–µ—Ä–µ–¥—å—é: {len(queue)} URL")
        else:
            queue = [(self.base_url, 0)]
            logger.info("–ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ")
        
        urls_explored = len(self.visited_urls)

        if session_urls:
            urls_explored = 0  # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        initial_strategy = "–≥–ª—É–±–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω)" if self.recipe_regex is None else "—à–∏—Ä–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω)"
        logger.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ö–æ–¥–∞: {initial_strategy}")

        err_count = 0  # –°—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥
        last_strategy = self.recipe_regex is not None  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π

        while queue and urls_explored < max_urls:
            # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ—Ç - –∏–¥–µ–º –≤–≥–ª—É–±—å (LIFO), –∏–Ω–∞—á–µ –≤—à–∏—Ä—å (FIFO)
            has_recipe_pattern = self.recipe_regex is not None
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if has_recipe_pattern != last_strategy:
                new_strategy = "—à–∏—Ä–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω)" if has_recipe_pattern else "–≥–ª—É–±–∏–Ω–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ—Ç–µ—Ä—è–Ω)"
                logger.info(f"‚ö° –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {new_strategy}")
                last_strategy = has_recipe_pattern
            
            # DFS: pop() –±–µ—Ä–µ—Ç —Å –∫–æ–Ω—Ü–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π - –ø–µ—Ä–≤—ã–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è)
            # BFS: pop(0) –±–µ—Ä–µ—Ç —Å –Ω–∞—á–∞–ª–∞ (–ø–µ—Ä–≤—ã–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π - –ø–µ—Ä–≤—ã–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è)
            current_url, depth = queue.pop() if not has_recipe_pattern else queue.pop(0)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã
            if depth > max_depth:
                continue
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            pattern = self.get_url_pattern(current_url)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –ø–æ—Å–µ—â–∞—Ç—å
            if not self.should_explore_url(current_url) and urls_explored > 0 and not check_pages_with_extractor:
                continue
            
            try:
                logger.info(f"[{urls_explored + 1}/{max_urls}] –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞: {current_url}")
                logger.info(f"  –ü–∞—Ç—Ç–µ—Ä–Ω: {pattern}, –ì–ª—É–±–∏–Ω–∞: {depth}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                try:
                    self.driver.get(current_url)
                except TimeoutException:
                    logger.warning(f"Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {current_url}")
                
                # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ (—Å–æ–∫—Ä–∞—â–µ–Ω–æ –¥–æ 15 —Å–µ–∫)
                try:
                    WebDriverWait(self.driver, 15).until(
                        lambda d: d.execute_script('return document.readyState') == 'complete'
                    )
                except TimeoutException:
                    logger.warning("Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: –∫–æ—Ä–æ—á–µ –≤ –Ω–∞—á–∞–ª–µ, –¥–ª–∏–Ω–Ω–µ–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 10 –∑–∞–ø—Ä–æ—Å–æ–≤
                self.request_count += 1
                if self.request_count % 10 == 0:
                    # –ö–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ - –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    delay = random.uniform(3, 5)
                    logger.info(f"  –î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ {self.request_count} –∑–∞–ø—Ä–æ—Å–æ–≤: {delay:.1f}—Å")
                else:
                    # –û–±—ã—á–Ω–∞—è –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
                    delay = random.uniform(0.8, 1.5)
                time.sleep(delay)
                
                # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
                use_quick_scroll = self.request_count % 3 != 0  # –ö–∞–∂–¥—ã–π 3-–π - –æ–±—ã—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
                self.slow_scroll_page(quick_mode=use_quick_scroll)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ
                self.visited_urls.add(current_url)
                urls_explored += 1
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω
                if pattern not in self.url_patterns:
                    self.url_patterns[pattern] = []
                
                page_index = len(self.url_patterns[pattern]) + 1
                self.url_patterns[pattern].append(current_url)
                
                
                # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω —Ä–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–¥–∞–Ω —Ä–µ–∂–∏–º –ø—Ä–æ–≤–µ–∫—Ä–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
                if check_pages_with_extractor and (check_url is False or self.should_extract_recipe(current_url)):   
                    if self.check_and_extract_recipe(current_url, pattern, page_index):
                        # –ï—Å–ª–∏ URL –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É, –Ω–æ —Ä–µ—Ü–µ–ø—Ç –Ω–∞–π–¥–µ–Ω - –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
                        if self.recipe_regex and not self.is_recipe_url(current_url):
                            logger.info("  –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ URL, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–π–¥–µ–Ω —Ä–µ—Ü–µ–ø—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                            if self.analyzer is None:
                                self.analyzer = RecipeAnalyzer(
                                    site_id=self.site_id,
                                    db_manager=self.db,
                                    sample_size=10
                                )
                            pattern =  self.analyzer.analyse_recipe_page_pattern(site_id=self.site_id)
                            
                # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω regex –ø–∞—Ç—Ç–µ—Ä–Ω - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—Ü–µ–ø—Ç, –∏–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                elif self.should_extract_recipe(current_url):
                    if not forbid_success_mark: self.mark_page_as_successful(current_url)
                    self.save_page_html(current_url, pattern, page_index)

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
                new_links = self.extract_links()
                logger.info(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(new_links)}")
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ –æ—á–µ—Ä–µ–¥—å —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ—Ü–µ–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –≥–ª—É–±–∏–Ω—É (DFS)
                # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —à–∏—Ä–∏–Ω—É (BFS)
                has_recipe_pattern = self.recipe_regex is not None
                
                for link in new_links:
                    if self.should_explore_url(link) or len(queue) == 0:
                        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–µ—Ä–µ—Ö–æ–¥–∞
                        if link not in self.referrer_map:
                            self.referrer_map[link] = current_url
                        
                        # DFS (–≤–≥–ª—É–±—å): –¥–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ –æ—á–µ—Ä–µ–¥–∏ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ—Ç
                        # BFS (–≤—à–∏—Ä—å): –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü –æ—á–µ—Ä–µ–¥–∏ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –µ—Å—Ç—å
                        if has_recipe_pattern:
                            queue.append((link, depth + 1))
                        else:
                            queue.insert(0, (link, depth + 1))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—á–µ—Ä–µ–¥—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞–π–¥–µ–Ω
                if has_recipe_pattern:
                    queue.sort(key=lambda x: self.get_url_priority(x[0]))
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if urls_explored % 10 == 0:
                    self.exploration_queue = queue  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –æ—á–µ—Ä–µ–¥—å
                    self.save_state()
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {current_url}: {e}")
                self.failed_urls.add(current_url)
                self.exploration_queue = queue  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                self.save_state()  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                err_count += 1
                if err_count >= self.max_errors:
                    logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥ ({self.max_errors}), –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.")
                    break
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–µ–π –æ—á–µ—Ä–µ–¥—å—é
        self.exploration_queue = queue
        self.save_state()
        
        logger.info(f"\n{'='*60}")
        logger.info("–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ" if err_count < self.max_errors else "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.save_dir}")
        logger.info(f"  - {self.state_file} - —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
        logger.info(f"  - {self.patterns_file} - –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        logger.info(f"  - *.html - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({sum(len(urls) for urls in self.url_patterns.values())} —Ñ–∞–π–ª–æ–≤)")
        logger.info("–î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: explorer.load_state() –∏–ª–∏ explorer.import_state(state)")
        logger.info(f"{'='*60}")
        return urls_explored

    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ –ë–î"""
        if self.driver and not self.debug_mode:
            self.driver.quit()
        if self.db:
            self.db.close()
        logger.info("–ì–æ—Ç–æ–≤–æ")


def explore_site(url: str, max_urls: int = 1000, max_depth: int = 4, recipe_pattern: str = None,
                 check_pages_with_extractor: bool = False,
                 forbid_success_mark: bool = False,
                 check_url: bool = False):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–∞–π—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π
    
    Args:
        explorer: –û–±—ä–µ–∫—Ç SiteExplorer
        max_urls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    """
    urls_explored = 0
    try:
        # –¶–∏–∫–ª –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è max_urls (–Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–æ–∫ –∏–ª–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π)
        while urls_explored < max_urls:
            explorer = SiteExplorer(url, debug_mode=True, use_db=True, recipe_pattern=recipe_pattern)
            explorer.connect_to_chrome()
            explorer.load_state()
            explored = explorer.explore(max_urls=max_urls, max_depth=max_depth, check_url=check_url, check_pages_with_extractor=check_pages_with_extractor, forbid_success_mark=forbid_success_mark)
            urls_explored += explored
            logger.info(f"–í—Å–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–æ URL: {urls_explored}/{max_urls}")
    except KeyboardInterrupt:
        logger.info("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)
    finally:
        explorer.close()

def main():
    url = "https://www.allrecipes.com/"
    # –ø–∞—Ç—Ç–µ—Ä–Ω —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—Å–∫–æ–ª–∫—å–∏—Ö URL
    search_pattern = "(^/recipe/\d+/[a-z0-9-]+/?$)|(^/[a-z0-9-]+-recipe-\d+/?$)"
    max_depth = 3
    
    explorer = SiteExplorer(url, debug_mode=True, use_db=True, recipe_pattern=search_pattern)
    
    try:
        #isR = explorer.is_recipe_url("https://www.allrecipes.com/recipe/23439/perfect-pumpkin-pie/")
        explorer.connect_to_chrome()
        explorer.explore(max_urls=3, max_depth=max_depth)

        explorer.explore(max_urls=3, max_depth=max_depth, session_urls=True)
    except KeyboardInterrupt:
        logger.info("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
    finally:
        explorer.close()


if __name__ == "__main__":
    main()
