## Задача: реализовать парсер рецептов для сайта alexanderlagarmat.se
---

Нужно реализовать новый парсер для сайта **alexanderlagarmat.se**.  
Парсер должен анализировать HTML‑страницы рецептов и извлекать данные в JSON в едином формате, который уже используется в проекте (как в парсере **allrecipes**).

---

### Контекст репозитория

- Язык проекта: Python.
- Парсеры рецептов находятся в директории: `extractor/`.
- Есть уже реализованный парсер для **allrecipes** — по нему можно ориентироваться:
  - базовый класс парсера;
  - структура возвращаемого JSON;
  - единый стиль кода (типизация, логирование и т.д.).

**Важно:** новый парсер должен вести себя максимально похоже на существующие (особенно на allrecipes) и быть прозрачно интегрируемым в текущий пайплайн.

---

### Входные данные

В репозитории уже сохранены примеры (директория `preprocessed/alexanderlagarmat_se`):

- HTML‑файлы страниц рецептов сайта **alexanderlagarmat.se**
- Соответствующие им JSON‑файлы с уже извлечёнными данными (эталон)

ВАЖНО:

- JSON‑файлы — это **только эталон / пример правильного результата**.
- **Их НЕЛЬЗЯ использовать напрямую в логике парсера**:
  - нельзя парсить JSON вместо HTML;
  - нельзя «подсматривать» значения из JSON в рантайме.
- JSON‑файлы нужны только для того, чтобы:
  - сверить корректность новой реализации;
  - убедиться, что структура и значения максимально совпадают.

---

### Требования к парсеру

#### 1. Файл и класс

- Создать новый модуль:

  - `extractor/alexanderlagarmat_se.py`

- В этом модуле реализовать класс парсера для **alexanderlagarmat.se**.
- Структура парсера должна:
  - соответствовать структуре парсера для **allrecipes**;
  - **наследоваться от того же базового класса**, что и парсер allrecipes  
    (`BaseExtractor`).
- Новый класс должен реализовать **те же публичные методы**, что и парсер allrecipes  
  (`extract_all()` - самый основной метод).

#### 2. Поведение парсера

- Парсер должен принимать HTML‑страницу рецепта:
  - уже реализовано в базовом классе, базовый класс BaseRecipeExtractor принимает путь к HTML файлу как строку,
- На выходе в методе extract_all() парсер должен **возвращать словарь (dict) / JSON‑совместимую структуру** с данными рецепта.
- Извлечение данных должно опираться на реальную HTML‑структуру **alexanderlagarmat.se**:
  - теги,
  - классы,
  - атрибуты,
  - возможный `microdata` / `JSON‑LD` и т.д.
- Логику парсинга необходимо строить **исключительно по HTML**, а не по структуре эталонного JSON.

---

### 3. Структура выходного JSON

Выходные данные парсера должны содержать **все** следующие поля (как в allrecipes), даже если в конкретном рецепте они отсутствуют:

- `dish_name`
- `description`
- `ingredients`
- `instructions`
- `category`
- `prep_time`
- `cook_time`
- `total_time`
- `notes`
- `image_urls`
- `tags`


**Обязательные правила:**

- Все перечисленные поля **обязаны присутствовать** в итоговом dict/JSON.
- Если значение не найдено на странице, оно **должно быть `None`**, а не отсутствовать в объекте.

**Форматы полей:**

- `dish_name`:  
  Строка — название блюда.

- `description`:  
  Строка — краткое текстовое описание рецепта (анонс/intro).

- `ingredients`:  
  Список ингредиентов.  
  Желательно проверить все места, в html файле, где упоминаются ингредиенты и брать их оттуда где name amount unit отдельно
  Формат каждого элемента: **словарь** с полями:
  - `name`: строка — название ингредиента (без количества);
  - `amount`: строка или число — количество (как представлено на сайте, можно нормализовать при необходимости);
  - `unit`: строка — единица измерения (например, `"g"`, `"ml"`, `"tbsp"`, `"cup"` и т.п.), если можно выделить.
  
  Пример структуры:
  ```json
  "ingredients": [
    {"name": "flour", "amount": "200", "unit": "g"},
    {"name": "butter", "amount": "50", "unit": "g"}
  ]
  ```

- `instructions`:  
  - строка со всеми щагами из рецепта

- `category`:  
  Строка или список строк — тип блюда / кухонная категория (например, `"Dessert"`, `"French"`, `"Main Course"`).

- `prep_time`:  
  Время подготовки.  
  Пример: `12 minutes`.

- `cook_time`:  
  Время готовки.  
  Пример: `45 minutes`.

- `total_time`:  
  Общее время приготовления.  
  Пример: `10 minutes`.

- `notes`:  
  Дополнительные заметки/комментарии к рецепту:
  - строка

- `tags`:
  Теги рецепта, формат - строка, теги разделены ` ,`
  Пример: `seafood main dishes, main dishes, seafood, fish, salmon`

- `image_urls`:  
  Список URL изображений рецепта (фотографии готового блюда).  
  Формат: строка, в которой url с изображениями перечислены через запятую без пробелов, может быть и одно изображение
  Пример:
  ```json
  "image_urls": "https://alexanderlagarmat.se/images/recipe1.jpg,https://alexanderlagarmat.se/images/recipe2.jpg"

  ```
  - Извлекать нужно все изображения, связанные с рецептом (основное фото блюда, пошаговые фото и т.д.).
  - Если изображений нет, поле должно быть пустым списком `` или `None`.

---

### 4. Сравнение с эталонным JSON

После реализации парсера нужно:

1. Для каждого HTML‑примера из `preprocessed/alexanderlagarmat_se`:
   - прогнать новый парсер;
   - получить результат в виде JSON/dict.

2. Сравнить результат с эталонным JSON:
   - структура (набор полей) должна совпадать;
   - значения полей — максимально близки по смыслу;
   - допустимы только косметические отличия:
     - пробелы,
     - переносы строк,
     - незначительные различия в форматировании текста.
  - допустимы отличия в поле image_urls (его может не быть в примере, но желательно, чтобы ссылки извлекались)

3. Убедиться, что **во всех случаях**:
   - отсутствующие на странице данные заполняются `None`;
   - формат типов (списки/строки/числа/словарь) совпадает с тем, что ожидает остальной код (см. allrecipes).

---

### 5. Технические детали реализации

- Использовать те же библиотеки и подходы к парсингу HTML, что и в остальной части проекта.
- Соблюдать:
  - типизацию (type hints),
  - стиль логирования (logging),
  - структуру классов и методов, принятую в проекте.
- По возможности делать парсер устойчивым к изменению верстки:
  - использовать стабильные селекторы (`data-*` атрибуты, семантические блоки, JSON‑LD),
  - избегать хрупких цепочек CSS‑классов.

- Обработка ошибок:
  - если структура страницы неожиданно изменилась или отсутствует часть данных, логировать проблему;
  - возвращать `None` в соответствующих полях, **не падать с исключением**.

- Точка входа `main()` в модуле `extractor/simplyrecipes_com.py`:
  - в модуле должен быть определён `def main():`, который:
    - ищет директорию с HTML‑страницами: `preprocessed/simplyrecipes_com` (относительно корня репозитория);
    - если директория существует, вызывает вспомогательную функцию проекта `process_directory(<ExtractorClass>, recipes_dir)` для обработки **всех** HTML‑файлов из этой папки;
    - гарантирует сохранение результатов парсинга в `_extracted`‑JSON‑файлы (по принятой в проекте схеме — рядом с исходными файлами или в соответствующей подпапке);
    - если директория не найдена, печатает понятное сообщение с путём и примером запуска.
  - Пример ожидаемого шаблона (адаптировать под конкретный класс парсера и simplyrecipes_com):
    ```python
    def main():
        import os

        from extractor.utils import process_directory  # если требуется, подправить импорт под реальный путь

        recipes_dir = os.path.join("preprocessed", "simplyrecipes_com")
        if os.path.exists(recipes_dir) and os.path.isdir(recipes_dir):
            # Замените MySiteExtractor на фактический класс парсера
            process_directory(MySiteExtractor, str(recipes_dir))
            return

        print(f"Директория не найдена: {recipes_dir}")
        print("Использование: python simplyrecipes_com.py")
    ```

  - Запуск `python extractor/simplyrecipes_com.py` должен быть достаточен, чтобы:
    - пройтись по всем HTML‑файлам из `preprocessed/simplyrecipes_com`;
    - сохранить для каждого файла соответствующий `_extracted`‑JSON с результатом парсинга.

---

### Ветка разработки и Pull Request

- Работать следует в отдельной ветке, например:
  - `feature/alexanderlagarmat_se-parser`
- Целевая ветка для Pull Request:
  - `feature/copilot-issue`.
- В PR указать:
  - ссылку на этот issue;
  - краткое описание реализованного парсера;
  - список файлов, затронутых изменениями (в первую очередь `extractor/alexanderlagarmat_se.py`);

---

### Критерии готовности задачи

1. В файле `extractor/alexanderlagarmat_se.py` создан класс парсера для **alexanderlagarmat.se**, наследующий базовый класс (как парсер allrecipes).
2. Метод(ы) парсера:
   - принимают HTML‑страницу рецепта alexanderlagarmat.se (путь к файлу, уже реализовано в базовом классе, нужно реализовать только остальные методы),
   - возвращают JSON/словарь со **всеми полями**, перечисленными выше,
   - заполняют отсутствующие данные значением `None`.
3. Формат выходного JSON **строго совместим** с форматом парсера allrecipes (по полям и типам данных).
4. Для предоставленных HTML‑примеров результат максимально близок к эталонным JSON‑файлам.
5. Парсер **не использует эталонные JSON‑файлы как источник данных**, а работает только с HTML.
6. Код следует общему стилю проекта, покрыт базовой обработкой ошибок и не ломает существующий пайплайн.
7. В модуле `extractor/simplyrecipes_com.py` определена рабочая функция `main()`, при запуске `python extractor/simplyrecipes_com.py`:
  - обрабатываются все HTML‑файлы из `preprocessed/simplyrecipes_com` с помощью реализованного класса парсера;
  - результаты сохранены в `_extracted`‑JSON‑файлы, по которым легко понять, какие страницы были успешно распарсены.
8. ОБЯЗАТЕЛЬНО ЗАПУСТИ main(), чтобы были созданы файлы _extracted.json для обоих рецептов в preprocessed/simplyrecipes_com/, эти файлы необходимы для review